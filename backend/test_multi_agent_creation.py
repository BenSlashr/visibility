#!/usr/bin/env python3
"""
Script de test pour la cr√©ation de prompts multi-agents
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from app.core.database import SessionLocal
from app.crud.prompt import crud_prompt
from app.crud.project import crud_project
from app.crud.ai_model import crud_ai_model
from app.schemas.prompt import PromptCreate
from sqlalchemy import text

def test_multi_agent_creation():
    """Test de cr√©ation d'un prompt multi-agents"""
    db = SessionLocal()
    
    try:
        print("üß™ Test de cr√©ation de prompt multi-agents")
        print("=" * 50)
        
        # 1. R√©cup√©rer un projet existant
        projects = crud_project.get_multi(db, limit=1)
        if not projects:
            print("‚ùå Aucun projet trouv√©")
            return
        
        project = projects[0]
        print(f"üìÅ Projet: {project.name} ({project.id[:8]}...)")
        
        # 2. R√©cup√©rer les mod√®les IA actifs
        all_models = crud_ai_model.get_multi(db)
        ai_models = [model for model in all_models if model.is_active]
        if len(ai_models) < 2:
            print(f"‚ùå Pas assez de mod√®les IA actifs ({len(ai_models)} trouv√©s)")
            return
        
        print(f"ü§ñ Mod√®les IA actifs: {len(ai_models)}")
        for model in ai_models[:3]:
            print(f"   - {model.name} ({model.provider})")
        
        # 3. Cr√©er un prompt multi-agents
        selected_models = ai_models[:2]  # Prendre les 2 premiers
        model_ids = [model.id for model in selected_models]
        
        prompt_data = PromptCreate(
            name="Test Multi-Agents Script",
            description="Prompt de test cr√©√© via script pour valider le multi-agents",
            template="Analysez {{query}} pour notre marque {{brand}} dans le secteur {{sector}}. Donnez votre avis sur {{topic}}.",
            project_id=project.id,
            is_multi_agent=True,
            ai_model_ids=model_ids,
            tags=["test", "multi-agents", "script"],
            is_active=True
        )
        
        print(f"\nüîß Cr√©ation du prompt multi-agents...")
        print(f"   - Nom: {prompt_data.name}")
        print(f"   - Mod√®les s√©lectionn√©s: {[m.name for m in selected_models]}")
        print(f"   - Mode multi-agent: {prompt_data.is_multi_agent}")
        
        # 4. Cr√©er le prompt
        new_prompt = crud_prompt.create_with_tags_and_models(db, obj_in=prompt_data)
        
        print(f"\n‚úÖ Prompt cr√©√© avec succ√®s!")
        print(f"   - ID: {new_prompt.id[:8]}...")
        print(f"   - Nom: {new_prompt.name}")
        print(f"   - Multi-agent: {new_prompt.is_multi_agent}")
        
        # 5. V√©rifier les relations cr√©√©es
        full_prompt = crud_prompt.get_with_relations(db, new_prompt.id)
        if full_prompt:
            print(f"   - Relations ai_models: {len(full_prompt.ai_models)}")
            for pam in full_prompt.ai_models:
                model_name = pam.ai_model.name if pam.ai_model else "Inconnu"
                print(f"     * {model_name} (actif: {pam.is_active})")
        
        # 6. V√©rifier dans la base de donn√©es
        count = db.execute(text("SELECT COUNT(*) FROM prompt_ai_models WHERE prompt_id = :prompt_id"), 
                          {"prompt_id": new_prompt.id}).scalar()
        print(f"   - Entr√©es en base: {count}")
        
        print(f"\nüéâ Test r√©ussi! Le prompt multi-agents fonctionne correctement.")
        
        return new_prompt.id
        
    except Exception as e:
        print(f"‚ùå Erreur lors du test: {e}")
        import traceback
        traceback.print_exc()
        return None
    finally:
        db.close()

if __name__ == "__main__":
    test_multi_agent_creation() 